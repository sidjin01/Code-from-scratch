# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-0yVe2tprZ46SJNyOpeRmZfTQLMl10gj

#RNN Architecture
* RNN can be understood with an anology of a number of resistors in series.
Here each neural network is a resistor. As in a series circuit, current across each resistor is same, similarly in a RNN, weights and bias across each neural network is same.
* The boxes in the figure below mark an individual neural network. The only difference is that along with inputs <I>x<sup>t</sup></I>, there is also another input <I>a<sup>t-1</I> which is nothing but the activations from the previous neural network's hidden layer.
* The flow of the notebook is as follows: 
  * We will first go forward for a single neural network the we'll go forward for all the neural networks whose number is defined as <I>T</I>; time steps, to get our predictions and activations for each neural network.
  * Then we will backpropagate a single neural network called a 'cell'. Then we will back propagate through all time steps to get our gradients.
  * Finally we will test our network on a simple data i.e we will create a random data <I>X</I> with 3 features and correspondingly we will generate <I>y</I> with two catagories 0 and 1 defined as: 0 if the sum of X<sup><I>i</I></sup> along all features is less than 0 and 1 if sum is greater than 0.
  
**The corresponing inputs and outputs of each function have been described in the function comments itself.**
"""

import numpy as np
#defining softmax
def softmax(x):
    e_x = np.exp(x)
    return e_x / e_x.sum(axis=0)

"""# RNN Feed Forward

<h2>Single step feed forward

![alt text](http://geyao1995.com/RNN_review/RNN_cell.png)
**<center>SINGLE TIME STEP FEED FORWARD</center>**
"""

def rnn_cell_forward(xt, a_prev, parameters):
    """
    Arguments:
    xt -- input data at timestep "t", numpy array of shape (n_x, m).
    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)
    parameters -- python dictionary containing:
        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
        ba --  Bias, numpy array of shape (n_a, 1)
        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)
    Returns:
    a_next -- next hidden state, of shape (n_a, m)
    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)
    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)
    """
    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]
    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)
    yt_pred = softmax(np.dot(Wya,a_next)+by)
    cache = (a_next, a_prev, xt, parameters)
    return a_next, yt_pred, cache

"""<h2>Feed forward through all time steps

![alt text](http://geyao1995.com/RNN_review/RNN_forward.png)





<center>RNN Feed Forward
"""

def rnn_forward(x, a0, parameters):
    """
    x -- Input data for every time-step, of shape (n_x, m, T_x).
    a0 -- Initial hidden state, of shape (n_a, m)
    Returns:
    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)
    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)
    caches -- tuple of values needed for the backward pass, contains (list of caches, x)
    """
    caches = []
    n_x, m, T_x = x.shape
    n_y, n_a = parameters["Wya"].shape
    a = np.zeros((n_a, m, T_x))
    y_pred = np.zeros((n_y, m, T_x))
    a_prev = a0
    
    for t in range(T_x):
        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_prev, parameters)
        a_prev= a_next
        a[:,:,t] = a_next
        y_pred[:,:,t] = yt_pred
        caches.append(cache)        
   # caches = (caches, x)    
    return a, y_pred, caches

"""#BPTT

<h3>Single time step backpropagation
"""

def singlecell_backprop(cache, yt_pred, Yt ):
    a_next, a_prev, xt, parameters = cache 
    Wya = parameters["Wya"]
    n_x,m = xt.shape
   
    #gradient of cost w.r.t z2
    dz2 =yt_pred - Yt 
    dWya = np.dot(dz2,a_next.T)
    da_next = np.dot(Wya.T, dz2)
    dby = np.sum(dz2, axis=1, keepdims = 1)
    dz1 = da_next*(1-a_next*a_next)
    dWaa = np.dot(dz1, a_prev.T)
    dWax = np.dot(dz1, xt.T)
    dba = np.sum(dz1, axis=1, keepdims = 1)
    gradients = {"dWya": dWya, "dby" : dby,  "dWax": dWax, "dWaa": dWaa, "dba": dba}  
    return gradients

"""<h3>Backpropagation through all time steps"""

def backprop(caches, y_pred, Y):
    a_next, a_prev, xt, parameters = caches[0] 
    n_a,m = a_next.shape
    n_x, m = xt.shape
    n_y,m,T_x = y_pred.shape
    dby = np.zeros((n_y,1))
    dWya = np.zeros((n_y,n_a))
    dba = np.zeros((n_a,m))
    dWax = np.zeros((n_a, n_x))
    dWaa = np.zeros((n_a, n_a))
    dba = np.zeros((n_a,1))
    for t in reversed(range(T_x)):
        gradients =  singlecell_backprop(caches[t], y_pred[:,:,t], Y[:,:,t] )
        dWya+=gradients["dWya"]
        dby+=gradients["dby"]
        dWax+=  gradients["dWax"]
        dWaa+=  gradients["dWaa"]
        dba+= gradients["dba"]
    gradients = {"dWya": dWya, "dby": dby, "dWax": dWax, "dWaa": dWaa,"dba": dba}
    return gradients

"""#Function to predict cost at each epoch"""

def cost(y_pred,Y):
    n_y,m,T_x = y_pred.shape
    cost=0
    for i in range(T_x):
        for row in range(m):
            cost =cost -np.dot(Y[:,row,i], np.transpose(np.log(y_pred[:,row,i]))) -np.dot((1-Y[:,row,i]),np.transpose(np.log(1-y_pred[:,row,i]))) 
    print("\r",cost/(m*T_x),end="")
    return None

"""#The main driver function
* Takes in the values of learning rate, the number of epochs to run and the initial activation.
"""

def RNN(X, Y, parameters, epochs, lr, a0):
    for i in range(epochs):
        a, y_pred, caches = rnn_forward(X, a0, parameters)
        gradients = backprop(caches, y_pred, Y)
    #updating the weights
        print("running epoch....",i+1)
        print("cost is :")
        cost(y_pred, Y)
        parameters['Wax'] += -lr * gradients['dWax']
        parameters['Waa'] += -lr * gradients['dWaa']
        parameters['Wya'] += -lr * gradients['dWya']
        parameters['ba']  += -lr * gradients['dba']
        parameters['by']  += -lr * gradients['dby']
    return y_pred

X = np.random.randn(3,4,2)
s = np.int32(np.sum(X, axis=0) > 0)  # 1 if sum > 0, else 0
# one-hot encoding of s along axis -1
# then move that axis to axis 0 cyclically, ie. (0, 1, 2) -> (2, 0, 1)
Y = np.moveaxis(np.eye(2)[s], -1, 0)
a0 = np.random.randn(5,4)
Waa = np.random.randn(5,5)*0.01
Wax = np.random.randn(5,3)*0.01
Wya = np.random.randn(2,5)*0.01
ba = np.random.randn(5,1)*0.01
by = np.random.randn(2,1)*0.01
parameters = {"Waa": Waa, "Wax": Wax, "Wya": Wya, "ba": ba, "by": by}

y_pred = RNN(X, Y, parameters, 100, .4, a0)

A=np.sum(abs(Y-y_pred))/(Y-y_pred).size

print("accuracy is:",(1-A)*100)

